{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz9HKe9_FTOp",
        "outputId": "90edb577-7e79-4760-b1fb-21ad8f72b3db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glq5khftFlEo",
        "outputId": "8827fa8f-7d8c-48f4-afe4-45ab15d79a89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313511 sha256=ad54d6fd41a64028e3b46136bcec49900fd06a94fa82fecd4330f830baaa064a\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IPN5zl0wFQQ6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import logging\n",
        "import fasttext\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers, models, preprocessing, optimizers\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "    def __init__(self, max_len: int = 64, embedding_dim: int = 300):\n",
        "        self.max_len = max_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.word2id = {'PAD': 0, 'UNK': 1}\n",
        "        self.id2word = {0: 'PAD', 1: 'UNK'}\n",
        "        self.label2id = {}\n",
        "        self.id2label = {}\n",
        "        self.model = None\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"load_data in process...\")\n",
        "        dataset = load_dataset(\"xtreme\", \"udpos.English\")\n",
        "\n",
        "        tags = dataset['train'].features['pos_tags'].feature.names\n",
        "        self.label2id = {tag: i for i, tag in enumerate(tags)}\n",
        "        self.id2label = {i: tag for i, tag in enumerate(tags)}\n",
        "\n",
        "        self._build_vocab(dataset['train']['tokens'])\n",
        "\n",
        "        X_train = self._vectorize(dataset['train']['tokens'])\n",
        "        y_train = self._pad_tags(dataset['train']['pos_tags'])\n",
        "\n",
        "        X_val = self._vectorize(dataset['validation']['tokens'])\n",
        "        y_val = self._pad_tags(dataset['validation']['pos_tags'])\n",
        "\n",
        "        X_test = self._vectorize(dataset['validation']['tokens'])\n",
        "        y_test = self._pad_tags(dataset['validation']['pos_tags'])\n",
        "\n",
        "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "    def _build_vocab(self, sentences: List[List[str]]):\n",
        "        vocab = Counter()\n",
        "        for sent in sentences:\n",
        "            vocab.update([word.lower() for word in sent])\n",
        "\n",
        "        for word in vocab:\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "            self.id2word[len(self.id2word)] = word\n",
        "\n",
        "    def _vectorize(self, sentences: List[List[str]]) -> np.ndarray:\n",
        "        vectorized = []\n",
        "        for sent in sentences:\n",
        "            ids = [self.word2id.get(word.lower(), 1) for word in sent]\n",
        "            vectorized.append(ids)\n",
        "        return preprocessing.sequence.pad_sequences(\n",
        "            vectorized, maxlen=self.max_len, padding='post'\n",
        "        )\n",
        "\n",
        "    def _pad_tags(self, tags: List[List[int]]) -> np.ndarray:\n",
        "        return preprocessing.sequence.pad_sequences(\n",
        "            tags, maxlen=self.max_len, value=0, padding='post'\n",
        "        )\n",
        "\n",
        "    def _build_embedding_matrix(self):\n",
        "        print(\"_build_embedding_matrix in process...\")\n",
        "        model_path = hf_hub_download(\"facebook/fasttext-en-vectors\", \"model.bin\")\n",
        "        ft_model = fasttext.load_model(model_path)\n",
        "\n",
        "        embedding_matrix = np.zeros((len(self.word2id), self.embedding_dim))\n",
        "        for word, idx in self.word2id.items():\n",
        "            if word in ft_model:\n",
        "                embedding_matrix[idx] = ft_model.get_word_vector(word)\n",
        "        return embedding_matrix\n",
        "\n",
        "    def build_model(self):\n",
        "        embedding_matrix = self._build_embedding_matrix()\n",
        "\n",
        "        inputs = layers.Input(shape=(self.max_len,))\n",
        "        x = layers.Embedding(\n",
        "            input_dim=len(self.word2id),\n",
        "            output_dim=self.embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            trainable=False\n",
        "        )(inputs)\n",
        "\n",
        "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "        outputs = layers.Dense(len(self.label2id), activation='softmax')(x)\n",
        "\n",
        "        self.model = models.Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        print(\"built successfully\\n\")\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs: int = 5, batch_size: int = 128):\n",
        "        print(\"Starting training...\\n\")\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.model.predict(X_test).argmax(axis=-1)\n",
        "        print(classification_report(\n",
        "            y_test.flatten(),\n",
        "            y_pred.flatten(),\n",
        "            target_names=list(self.label2id.keys()),\n",
        "            zero_division=0\n",
        "        ))\n",
        "\n",
        "    def predict_text(self, text: str) -> List[Tuple[str, str]]:\n",
        "        tokens = re.findall(r'\\w+|[^\\w\\s]+', text)\n",
        "        token_ids = [self.word2id.get(token.lower(), 1) for token in tokens]\n",
        "        padded_ids = preprocessing.sequence.pad_sequences(\n",
        "            [token_ids], maxlen=self.max_len, padding='post'\n",
        "        )\n",
        "\n",
        "        preds = self.model.predict(padded_ids)[0]\n",
        "        tags = [self.id2label[pred.argmax()] for pred in preds[:len(tokens)]]\n",
        "\n",
        "        return list(zip(tokens, tags))"
      ],
      "metadata": {
        "id": "CbwfY6tum7LQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    tagger = POSTagger(max_len=64)\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = tagger.load_data()\n",
        "    tagger.build_model()\n",
        "    tagger.train(X_train, y_train, X_val, y_val, epochs=5)\n",
        "    tagger.evaluate(X_test, y_test)\n",
        "\n",
        "    test_sentence = \"The Michael Book will book, a book, for us.\"\n",
        "    tagged = tagger.predict_text(test_sentence)\n",
        "    print(\"\\nPredicted tags:\")\n",
        "    for token, tag in tagged:\n",
        "        print(f\"{token:15} -- {tag}\")"
      ],
      "metadata": {
        "id": "r7DfbCZ3-vk-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TduyFA5MFaoJ",
        "outputId": "b1cffb20-827b-4951-f4e8-eda6f094e234"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_data in process...\n",
            "_build_embedding_matrix in process...\n",
            "built successfully\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 552ms/step - accuracy: 0.7418 - loss: 0.9014 - val_accuracy: 0.8798 - val_loss: 0.3957\n",
            "Epoch 2/5\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 531ms/step - accuracy: 0.8892 - loss: 0.3678 - val_accuracy: 0.9467 - val_loss: 0.1778\n",
            "Epoch 3/5\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 519ms/step - accuracy: 0.9444 - loss: 0.1843 - val_accuracy: 0.9613 - val_loss: 0.1285\n",
            "Epoch 4/5\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 511ms/step - accuracy: 0.9606 - loss: 0.1293 - val_accuracy: 0.9660 - val_loss: 0.1147\n",
            "Epoch 5/5\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 527ms/step - accuracy: 0.9679 - loss: 0.1050 - val_accuracy: 0.9674 - val_loss: 0.1106\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 68ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       1.00      0.99      1.00    196242\n",
            "         ADP       0.92      0.95      0.93      5845\n",
            "         ADV       0.75      0.71      0.73      2959\n",
            "         AUX       0.90      0.95      0.92      3404\n",
            "       CCONJ       0.99      0.98      0.99      1991\n",
            "         DET       0.97      0.96      0.97      5267\n",
            "        INTJ       0.92      0.07      0.12       166\n",
            "        NOUN       0.90      0.86      0.88     11081\n",
            "         NUM       0.28      0.95      0.44       880\n",
            "        PART       0.94      0.90      0.92      1536\n",
            "        PRON       0.83      0.95      0.89      5428\n",
            "       PROPN       0.79      0.53      0.63      3540\n",
            "       PUNCT       0.98      1.00      0.99      7720\n",
            "       SCONJ       0.76      0.64      0.70      1061\n",
            "         SYM       0.58      0.29      0.39        89\n",
            "        VERB       0.89      0.84      0.86      6913\n",
            "           X       0.00      0.00      0.00       214\n",
            "\n",
            "    accuracy                           0.97    254336\n",
            "   macro avg       0.79      0.74      0.73    254336\n",
            "weighted avg       0.97      0.97      0.97    254336\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "Predicted tags:\n",
            "The             -- DET\n",
            "Michael         -- PROPN\n",
            "Book            -- NOUN\n",
            "will            -- AUX\n",
            "book            -- NOUN\n",
            ",               -- PUNCT\n",
            "a               -- DET\n",
            "book            -- NOUN\n",
            ",               -- PUNCT\n",
            "for             -- ADP\n",
            "us              -- PRON\n",
            ".               -- PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMpRu7frpFtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}